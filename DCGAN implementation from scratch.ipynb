{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DCGAN model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, channels_imag, features_d):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            nn.Conv2d(channels_imag, features_d, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            self._block(features_d, features_d * 2, 4, 2, 1),\n",
    "            self._block(features_d * 2, features_d * 4, 4, 2, 1),\n",
    "            self._block(features_d * 4, features_d * 8, 4, 2, 1),\n",
    "            nn.Conv2d(features_d * 8, 1, kernel_size=4, stride=2, padding=0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.disc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, channels_img, features_g):\n",
    "        super(Generator, self).__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            self._block(z_dim, features_g*16, 4, 1, 0),\n",
    "            self._block(features_g*16, features_g*8, 4, 2, 1),\n",
    "            self._block(features_g*8, features_g*4, 4, 2, 1),\n",
    "            self._block(features_g*4, features_g*2, 4, 2, 1),\n",
    "            nn.ConvTranspose2d(features_g*2, channels_img, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )                    \n",
    "\n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "    )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.gen(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(model):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n",
    "            nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    N, in_channels, H, W = 8, 3, 64, 64\n",
    "    z_dim = 100\n",
    "    x = torch.randn((N, in_channels, H, W))\n",
    "    disc = Discriminator(in_channels, 8)\n",
    "    initialize_weights(disc)\n",
    "    assert disc(x).shape == (N, 1, 1, 1)\n",
    "    gen = Generator(z_dim, in_channels, 8)\n",
    "    initialize_weights(gen)\n",
    "    z = torch.randn((N, z_dim , 1, 1 ))\n",
    "    assert gen(z).shape == (N, in_channels, H, W)\n",
    "    print(\"Successfully initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully initialized\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri May 10 12:41:56 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 552.22                 Driver Version: 552.22         CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                     TCC/WDDM  | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3050 ...  WDDM  |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   61C    P0             15W /   71W |       0MiB /   4096MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fyzan\\anaconda3\\envs\\isl\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# from model import Discriminator, Generator, initialize_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settign the hyperparameters\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "LEARNING_RATE = 2e-4\n",
    "BATCH_SIZE = 128\n",
    "IMAGE_SIZE = 64\n",
    "# CHANNELS_IMG = 1\n",
    "CHANNELS_IMG = 3\n",
    "Z_DIM = 100\n",
    "NUM_EPOCHS = 10\n",
    "FEATURES_DISC = 64\n",
    "FEATURES_GEN = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transforms = transforms.Compose([\n",
    "    transforms.Resize(IMAGE_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        [0.5 for _ in range(CHANNELS_IMG)], [0.5 for _ in range(CHANNELS_IMG)]),\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = datasets.MNIST(\n",
    "#     root=\"dataset/\", train=True, transform=transforms, download=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.ImageFolder(\n",
    "    root=\"celeb_dataset\", transform=transforms )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "gen = Generator(Z_DIM, CHANNELS_IMG, FEATURES_GEN).to(device)\n",
    "disc = Discriminator(CHANNELS_IMG, FEATURES_DISC).to(device)\n",
    "\n",
    "initialize_weights(gen)\n",
    "initialize_weights(disc)\n",
    "\n",
    "opt_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
    "opt_disc = optim.Adam(disc.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999)) \n",
    "criterion = nn.BCELoss()\n",
    "fixed_noise = torch.randn(32, Z_DIM, 1, 1).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer_real = SummaryWriter(f\"logs/real celeb\")\n",
    "writer_fake = SummaryWriter(f\"logs/fake celeb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (disc): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (2): Sequential(\n",
       "      (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    )\n",
       "    (5): Conv2d(512, 1, kernel_size=(4, 4), stride=(2, 2), bias=False)\n",
       "    (6): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step = 0\n",
    "\n",
    "gen.train()\n",
    "disc.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    for batch_idx, (real, _) in enumerate(loader):\n",
    "        real = real.to(device)\n",
    "        noise = torch.randn(BATCH_SIZE, Z_DIM, 1, 1).to(device)\n",
    "        fake = gen(noise)\n",
    "\n",
    "        ### Train Discriminator: max log(D(x)) + log(1 - D(G(z)))\n",
    "        disc.zero_grad()\n",
    "        disc_real = disc(real).reshape(-1)\n",
    "        loss_disc_real = criterion(disc_real, torch.ones_like(disc_real))\n",
    "        disc_fake = disc(fake.detach()).reshape(-1)\n",
    "        loss_disc_fake = criterion(disc_fake, torch.zeros_like(disc_fake))\n",
    "        loss_disc = (loss_disc_real + loss_disc_fake) / 2\n",
    "        loss_disc.backward(retain_graph=True)\n",
    "        opt_disc.step()\n",
    "\n",
    "        ### Train Generator: min log(1 - D(G(z))) <-> max log(D(G(z)))\n",
    "        gen.zero_grad()\n",
    "        output = disc(fake).reshape(-1)\n",
    "        loss_gen = criterion(output, torch.ones_like(output))\n",
    "        loss_gen.backward()\n",
    "        opt_gen.step()\n",
    "\n",
    "        # print losses occassionally and print to tensorboard\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch}/{NUM_EPOCHS}] Batch {batch_idx}/{len(loader)} \\\n",
    "                  Loss D: {loss_disc:.4f}, loss G: {loss_gen:.4f}\"\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                fake = gen(fixed_noise)\n",
    "                img_grid_real = torchvision.utils.make_grid(\n",
    "                    real[:32], normalize=True\n",
    "                )\n",
    "                img_grid_fake = torchvision.utils.make_grid(\n",
    "                    fake[:32], normalize=True\n",
    "                )\n",
    "                writer_real.add_image(\"Real Celeb\", img_grid_real, global_step=step)\n",
    "                writer_fake.add_image(\"Fake Celeb\", img_grid_fake, global_step=step)\n",
    "\n",
    "            step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/10] Batch 0/1583                   Loss D: 0.3771, loss G: 1.2661\n",
      "Epoch [0/10] Batch 10/1583                   Loss D: 0.1890, loss G: 1.8192\n",
      "Epoch [0/10] Batch 20/1583                   Loss D: 0.1124, loss G: 2.2851\n",
      "Epoch [0/10] Batch 30/1583                   Loss D: 0.0729, loss G: 2.6861\n",
      "Epoch [0/10] Batch 40/1583                   Loss D: 0.0504, loss G: 3.0385\n",
      "Epoch [0/10] Batch 50/1583                   Loss D: 0.0377, loss G: 3.3243\n",
      "Epoch [0/10] Batch 60/1583                   Loss D: 0.0290, loss G: 3.5926\n",
      "Epoch [0/10] Batch 70/1583                   Loss D: 0.0211, loss G: 3.8458\n",
      "Epoch [0/10] Batch 80/1583                   Loss D: 0.0171, loss G: 4.0526\n",
      "Epoch [0/10] Batch 90/1583                   Loss D: 0.0143, loss G: 4.2526\n",
      "Epoch [0/10] Batch 100/1583                   Loss D: 0.0118, loss G: 4.4028\n",
      "Epoch [0/10] Batch 110/1583                   Loss D: 0.0100, loss G: 4.5888\n",
      "Epoch [0/10] Batch 120/1583                   Loss D: 0.0085, loss G: 4.7303\n",
      "Epoch [0/10] Batch 130/1583                   Loss D: 0.0080, loss G: 4.8744\n",
      "Epoch [0/10] Batch 140/1583                   Loss D: 0.0069, loss G: 5.0151\n",
      "Epoch [0/10] Batch 150/1583                   Loss D: 0.0064, loss G: 5.1595\n",
      "Epoch [0/10] Batch 160/1583                   Loss D: 0.0255, loss G: 4.0519\n",
      "Epoch [0/10] Batch 170/1583                   Loss D: 0.0131, loss G: 4.4718\n",
      "Epoch [0/10] Batch 180/1583                   Loss D: 0.0111, loss G: 4.8058\n",
      "Epoch [0/10] Batch 190/1583                   Loss D: 0.0100, loss G: 5.1003\n",
      "Epoch [0/10] Batch 200/1583                   Loss D: 0.0226, loss G: 5.4267\n",
      "Epoch [0/10] Batch 210/1583                   Loss D: 0.0123, loss G: 4.9045\n",
      "Epoch [0/10] Batch 220/1583                   Loss D: 0.0102, loss G: 5.0828\n",
      "Epoch [0/10] Batch 230/1583                   Loss D: 0.0065, loss G: 5.3825\n",
      "Epoch [0/10] Batch 240/1583                   Loss D: 0.0051, loss G: 5.5381\n",
      "Epoch [0/10] Batch 250/1583                   Loss D: 0.0043, loss G: 5.7000\n",
      "Epoch [0/10] Batch 260/1583                   Loss D: 0.0035, loss G: 5.7373\n",
      "Epoch [0/10] Batch 270/1583                   Loss D: 0.0033, loss G: 5.7618\n",
      "Epoch [0/10] Batch 280/1583                   Loss D: 0.0039, loss G: 5.7402\n",
      "Epoch [0/10] Batch 290/1583                   Loss D: 0.0043, loss G: 5.7887\n",
      "Epoch [0/10] Batch 300/1583                   Loss D: 0.0037, loss G: 5.8606\n",
      "Epoch [0/10] Batch 310/1583                   Loss D: 0.0035, loss G: 5.8036\n",
      "Epoch [0/10] Batch 320/1583                   Loss D: 0.0040, loss G: 5.8250\n",
      "Epoch [0/10] Batch 330/1583                   Loss D: 0.0029, loss G: 6.0799\n",
      "Epoch [0/10] Batch 340/1583                   Loss D: 0.0032, loss G: 5.9733\n",
      "Epoch [0/10] Batch 350/1583                   Loss D: 0.0028, loss G: 6.0628\n",
      "Epoch [0/10] Batch 360/1583                   Loss D: 0.0024, loss G: 6.1522\n",
      "Epoch [0/10] Batch 370/1583                   Loss D: 0.0024, loss G: 6.1974\n",
      "Epoch [0/10] Batch 380/1583                   Loss D: 0.0021, loss G: 6.2386\n",
      "Epoch [0/10] Batch 390/1583                   Loss D: 0.0019, loss G: 6.3069\n",
      "Epoch [0/10] Batch 400/1583                   Loss D: 0.0016, loss G: 6.4167\n",
      "Epoch [0/10] Batch 410/1583                   Loss D: 0.0016, loss G: 6.4326\n",
      "Epoch [0/10] Batch 420/1583                   Loss D: 0.0015, loss G: 6.4748\n",
      "Epoch [0/10] Batch 430/1583                   Loss D: 0.0014, loss G: 6.5261\n",
      "Epoch [0/10] Batch 440/1583                   Loss D: 0.0013, loss G: 6.5429\n",
      "Epoch [0/10] Batch 450/1583                   Loss D: 0.0012, loss G: 6.5749\n",
      "Epoch [0/10] Batch 460/1583                   Loss D: 0.0013, loss G: 6.5667\n",
      "Epoch [0/10] Batch 470/1583                   Loss D: 0.0011, loss G: 6.6723\n",
      "Epoch [0/10] Batch 480/1583                   Loss D: 0.0010, loss G: 6.8134\n",
      "Epoch [0/10] Batch 490/1583                   Loss D: 0.0011, loss G: 6.7336\n",
      "Epoch [0/10] Batch 500/1583                   Loss D: 0.0010, loss G: 6.7780\n",
      "Epoch [0/10] Batch 510/1583                   Loss D: 0.0010, loss G: 6.8446\n",
      "Epoch [0/10] Batch 520/1583                   Loss D: 0.0009, loss G: 6.9208\n",
      "Epoch [0/10] Batch 530/1583                   Loss D: 0.0008, loss G: 6.9953\n",
      "Epoch [0/10] Batch 540/1583                   Loss D: 0.0008, loss G: 7.0301\n",
      "Epoch [0/10] Batch 550/1583                   Loss D: 0.0007, loss G: 7.1370\n",
      "Epoch [0/10] Batch 560/1583                   Loss D: 0.0007, loss G: 7.2162\n",
      "Epoch [0/10] Batch 570/1583                   Loss D: 0.0006, loss G: 7.2905\n",
      "Epoch [0/10] Batch 580/1583                   Loss D: 0.0006, loss G: 7.3303\n",
      "Epoch [0/10] Batch 590/1583                   Loss D: 0.0006, loss G: 7.3224\n",
      "Epoch [0/10] Batch 600/1583                   Loss D: 0.0006, loss G: 7.3615\n",
      "Epoch [0/10] Batch 610/1583                   Loss D: 0.0006, loss G: 7.3128\n",
      "Epoch [0/10] Batch 620/1583                   Loss D: 0.0005, loss G: 7.3485\n",
      "Epoch [0/10] Batch 630/1583                   Loss D: 0.0006, loss G: 7.3015\n",
      "Epoch [0/10] Batch 640/1583                   Loss D: 0.0006, loss G: 7.3270\n",
      "Epoch [0/10] Batch 650/1583                   Loss D: 0.0005, loss G: 7.3121\n",
      "Epoch [0/10] Batch 660/1583                   Loss D: 0.0006, loss G: 7.3123\n",
      "Epoch [0/10] Batch 670/1583                   Loss D: 0.0006, loss G: 7.3789\n",
      "Epoch [0/10] Batch 680/1583                   Loss D: 0.0005, loss G: 7.4530\n",
      "Epoch [0/10] Batch 690/1583                   Loss D: 0.0005, loss G: 7.4600\n",
      "Epoch [0/10] Batch 700/1583                   Loss D: 0.0005, loss G: 7.5199\n",
      "Epoch [0/10] Batch 710/1583                   Loss D: 0.0005, loss G: 7.5217\n",
      "Epoch [0/10] Batch 720/1583                   Loss D: 0.0005, loss G: 7.4926\n",
      "Epoch [0/10] Batch 730/1583                   Loss D: 0.0004, loss G: 7.5996\n",
      "Epoch [0/10] Batch 740/1583                   Loss D: 0.0004, loss G: 7.6454\n",
      "Epoch [0/10] Batch 750/1583                   Loss D: 0.0004, loss G: 7.7219\n",
      "Epoch [0/10] Batch 760/1583                   Loss D: 0.0004, loss G: 7.7482\n",
      "Epoch [0/10] Batch 770/1583                   Loss D: 0.0003, loss G: 7.8381\n",
      "Epoch [0/10] Batch 780/1583                   Loss D: 0.0003, loss G: 7.8961\n",
      "Epoch [0/10] Batch 790/1583                   Loss D: 0.0003, loss G: 7.9844\n",
      "Epoch [0/10] Batch 800/1583                   Loss D: 0.0003, loss G: 7.9871\n",
      "Epoch [0/10] Batch 810/1583                   Loss D: 0.0003, loss G: 8.0318\n",
      "Epoch [0/10] Batch 820/1583                   Loss D: 0.0003, loss G: 8.0680\n",
      "Epoch [0/10] Batch 830/1583                   Loss D: 0.0003, loss G: 8.1321\n",
      "Epoch [0/10] Batch 840/1583                   Loss D: 0.0003, loss G: 8.1381\n",
      "Epoch [0/10] Batch 850/1583                   Loss D: 0.0003, loss G: 8.2017\n",
      "Epoch [0/10] Batch 860/1583                   Loss D: 0.0003, loss G: 8.2543\n",
      "Epoch [0/10] Batch 870/1583                   Loss D: 0.0003, loss G: 8.2728\n",
      "Epoch [0/10] Batch 880/1583                   Loss D: 0.0002, loss G: 8.3036\n",
      "Epoch [0/10] Batch 890/1583                   Loss D: 0.0002, loss G: 8.3163\n",
      "Epoch [0/10] Batch 900/1583                   Loss D: 0.0002, loss G: 8.3678\n",
      "Epoch [0/10] Batch 910/1583                   Loss D: 0.0002, loss G: 8.4073\n",
      "Epoch [0/10] Batch 920/1583                   Loss D: 0.0002, loss G: 8.4096\n",
      "Epoch [0/10] Batch 930/1583                   Loss D: 0.0002, loss G: 8.4183\n",
      "Epoch [0/10] Batch 940/1583                   Loss D: 0.0002, loss G: 8.4211\n",
      "Epoch [0/10] Batch 950/1583                   Loss D: 0.0002, loss G: 8.4173\n",
      "Epoch [0/10] Batch 960/1583                   Loss D: 0.0002, loss G: 8.4432\n",
      "Epoch [0/10] Batch 970/1583                   Loss D: 0.0002, loss G: 8.3990\n",
      "Epoch [0/10] Batch 980/1583                   Loss D: 0.0002, loss G: 8.4140\n",
      "Epoch [0/10] Batch 990/1583                   Loss D: 0.0002, loss G: 8.4026\n",
      "Epoch [0/10] Batch 1000/1583                   Loss D: 0.0002, loss G: 8.4523\n",
      "Epoch [0/10] Batch 1010/1583                   Loss D: 0.0002, loss G: 8.4402\n",
      "Epoch [0/10] Batch 1020/1583                   Loss D: 0.0002, loss G: 8.4470\n",
      "Epoch [0/10] Batch 1030/1583                   Loss D: 0.0002, loss G: 8.4789\n",
      "Epoch [0/10] Batch 1040/1583                   Loss D: 0.0002, loss G: 8.4780\n",
      "Epoch [0/10] Batch 1050/1583                   Loss D: 0.0002, loss G: 8.4791\n",
      "Epoch [0/10] Batch 1060/1583                   Loss D: 0.0002, loss G: 8.5089\n",
      "Epoch [0/10] Batch 1070/1583                   Loss D: 0.0002, loss G: 8.5220\n",
      "Epoch [0/10] Batch 1080/1583                   Loss D: 0.0002, loss G: 8.5554\n",
      "Epoch [0/10] Batch 1090/1583                   Loss D: 0.0002, loss G: 8.6082\n",
      "Epoch [0/10] Batch 1100/1583                   Loss D: 0.0002, loss G: 8.5924\n",
      "Epoch [0/10] Batch 1110/1583                   Loss D: 0.0002, loss G: 8.6472\n",
      "Epoch [0/10] Batch 1120/1583                   Loss D: 0.0002, loss G: 8.6441\n",
      "Epoch [0/10] Batch 1130/1583                   Loss D: 0.0002, loss G: 8.6864\n",
      "Epoch [0/10] Batch 1140/1583                   Loss D: 0.0001, loss G: 8.6863\n",
      "Epoch [0/10] Batch 1150/1583                   Loss D: 0.0002, loss G: 8.7061\n",
      "Epoch [0/10] Batch 1160/1583                   Loss D: 0.0001, loss G: 8.7446\n",
      "Epoch [0/10] Batch 1170/1583                   Loss D: 0.0002, loss G: 8.7375\n",
      "Epoch [0/10] Batch 1180/1583                   Loss D: 0.0001, loss G: 8.7422\n",
      "Epoch [0/10] Batch 1190/1583                   Loss D: 0.0001, loss G: 8.7839\n",
      "Epoch [0/10] Batch 1200/1583                   Loss D: 0.0001, loss G: 8.7898\n",
      "Epoch [0/10] Batch 1210/1583                   Loss D: 0.0001, loss G: 8.7816\n",
      "Epoch [0/10] Batch 1220/1583                   Loss D: 0.0001, loss G: 8.7799\n",
      "Epoch [0/10] Batch 1230/1583                   Loss D: 0.0001, loss G: 8.8038\n",
      "Epoch [0/10] Batch 1240/1583                   Loss D: 0.0001, loss G: 8.8219\n",
      "Epoch [0/10] Batch 1250/1583                   Loss D: 0.0001, loss G: 8.8050\n",
      "Epoch [0/10] Batch 1260/1583                   Loss D: 0.0001, loss G: 8.8254\n",
      "Epoch [0/10] Batch 1270/1583                   Loss D: 0.0001, loss G: 8.8823\n",
      "Epoch [0/10] Batch 1280/1583                   Loss D: 0.0001, loss G: 8.8630\n",
      "Epoch [0/10] Batch 1290/1583                   Loss D: 0.0001, loss G: 8.8959\n",
      "Epoch [0/10] Batch 1300/1583                   Loss D: 0.0001, loss G: 8.9287\n",
      "Epoch [0/10] Batch 1310/1583                   Loss D: 0.0001, loss G: 8.9571\n",
      "Epoch [0/10] Batch 1320/1583                   Loss D: 0.0001, loss G: 8.9978\n",
      "Epoch [0/10] Batch 1330/1583                   Loss D: 0.0001, loss G: 9.0112\n",
      "Epoch [0/10] Batch 1340/1583                   Loss D: 0.0001, loss G: 9.0391\n",
      "Epoch [0/10] Batch 1350/1583                   Loss D: 0.0001, loss G: 9.0587\n",
      "Epoch [0/10] Batch 1360/1583                   Loss D: 0.0001, loss G: 9.1044\n",
      "Epoch [0/10] Batch 1370/1583                   Loss D: 0.0001, loss G: 9.1161\n",
      "Epoch [0/10] Batch 1380/1583                   Loss D: 0.0001, loss G: 9.1389\n",
      "Epoch [0/10] Batch 1390/1583                   Loss D: 0.0001, loss G: 9.1699\n",
      "Epoch [0/10] Batch 1400/1583                   Loss D: 0.0001, loss G: 9.1770\n",
      "Epoch [0/10] Batch 1410/1583                   Loss D: 0.0001, loss G: 9.2035\n",
      "Epoch [0/10] Batch 1420/1583                   Loss D: 0.0001, loss G: 9.2286\n",
      "Epoch [0/10] Batch 1430/1583                   Loss D: 0.0001, loss G: 9.2550\n",
      "Epoch [0/10] Batch 1440/1583                   Loss D: 0.0001, loss G: 9.2701\n",
      "Epoch [0/10] Batch 1450/1583                   Loss D: 0.0001, loss G: 9.2876\n",
      "Epoch [0/10] Batch 1460/1583                   Loss D: 0.0001, loss G: 9.3065\n",
      "Epoch [0/10] Batch 1470/1583                   Loss D: 0.0001, loss G: 9.3181\n",
      "Epoch [0/10] Batch 1480/1583                   Loss D: 0.0001, loss G: 9.3361\n",
      "Epoch [0/10] Batch 1490/1583                   Loss D: 0.0001, loss G: 9.3719\n",
      "Epoch [0/10] Batch 1500/1583                   Loss D: 0.0001, loss G: 9.3817\n",
      "Epoch [0/10] Batch 1510/1583                   Loss D: 0.0001, loss G: 9.3934\n",
      "Epoch [0/10] Batch 1520/1583                   Loss D: 0.0001, loss G: 9.4170\n",
      "Epoch [0/10] Batch 1530/1583                   Loss D: 0.0001, loss G: 9.4288\n",
      "Epoch [0/10] Batch 1540/1583                   Loss D: 0.0001, loss G: 9.4248\n",
      "Epoch [0/10] Batch 1550/1583                   Loss D: 0.0001, loss G: 9.4554\n",
      "Epoch [0/10] Batch 1560/1583                   Loss D: 0.0001, loss G: 9.4686\n",
      "Epoch [0/10] Batch 1570/1583                   Loss D: 0.0001, loss G: 9.4722\n",
      "Epoch [0/10] Batch 1580/1583                   Loss D: 0.0001, loss G: 9.4894\n",
      "Epoch [1/10] Batch 0/1583                   Loss D: 0.0001, loss G: 9.5020\n",
      "Epoch [1/10] Batch 10/1583                   Loss D: 0.0001, loss G: 9.5123\n",
      "Epoch [1/10] Batch 20/1583                   Loss D: 0.0001, loss G: 9.5482\n",
      "Epoch [1/10] Batch 30/1583                   Loss D: 0.0001, loss G: 9.5416\n",
      "Epoch [1/10] Batch 40/1583                   Loss D: 0.0001, loss G: 9.5555\n",
      "Epoch [1/10] Batch 50/1583                   Loss D: 0.0001, loss G: 9.5623\n",
      "Epoch [1/10] Batch 60/1583                   Loss D: 0.0001, loss G: 9.5807\n",
      "Epoch [1/10] Batch 70/1583                   Loss D: 0.0001, loss G: 9.5964\n",
      "Epoch [1/10] Batch 80/1583                   Loss D: 0.0001, loss G: 9.6110\n",
      "Epoch [1/10] Batch 90/1583                   Loss D: 0.0001, loss G: 9.6155\n",
      "Epoch [1/10] Batch 100/1583                   Loss D: 0.0001, loss G: 9.6264\n",
      "Epoch [1/10] Batch 110/1583                   Loss D: 0.0001, loss G: 9.6425\n",
      "Epoch [1/10] Batch 120/1583                   Loss D: 0.0001, loss G: 9.6547\n",
      "Epoch [1/10] Batch 130/1583                   Loss D: 0.0001, loss G: 9.6723\n",
      "Epoch [1/10] Batch 140/1583                   Loss D: 0.0001, loss G: 9.6841\n",
      "Epoch [1/10] Batch 150/1583                   Loss D: 0.0001, loss G: 9.6996\n",
      "Epoch [1/10] Batch 160/1583                   Loss D: 0.0001, loss G: 9.7164\n",
      "Epoch [1/10] Batch 170/1583                   Loss D: 0.0001, loss G: 9.7229\n",
      "Epoch [1/10] Batch 180/1583                   Loss D: 0.0001, loss G: 9.7225\n",
      "Epoch [1/10] Batch 190/1583                   Loss D: 0.0001, loss G: 9.7336\n",
      "Epoch [1/10] Batch 200/1583                   Loss D: 0.0001, loss G: 9.7366\n",
      "Epoch [1/10] Batch 210/1583                   Loss D: 0.0001, loss G: 9.7372\n",
      "Epoch [1/10] Batch 220/1583                   Loss D: 0.0001, loss G: 9.7390\n",
      "Epoch [1/10] Batch 230/1583                   Loss D: 0.0001, loss G: 9.7258\n",
      "Epoch [1/10] Batch 240/1583                   Loss D: 0.0001, loss G: 9.7349\n",
      "Epoch [1/10] Batch 250/1583                   Loss D: 0.0001, loss G: 9.7434\n",
      "Epoch [1/10] Batch 260/1583                   Loss D: 0.0001, loss G: 9.7377\n",
      "Epoch [1/10] Batch 270/1583                   Loss D: 0.0001, loss G: 9.7416\n",
      "Epoch [1/10] Batch 280/1583                   Loss D: 0.0001, loss G: 9.7592\n",
      "Epoch [1/10] Batch 290/1583                   Loss D: 0.0001, loss G: 9.7501\n",
      "Epoch [1/10] Batch 300/1583                   Loss D: 0.0001, loss G: 9.7410\n",
      "Epoch [1/10] Batch 310/1583                   Loss D: 0.0001, loss G: 9.7570\n",
      "Epoch [1/10] Batch 320/1583                   Loss D: 0.0001, loss G: 9.7537\n",
      "Epoch [1/10] Batch 330/1583                   Loss D: 0.0001, loss G: 9.7695\n",
      "Epoch [1/10] Batch 340/1583                   Loss D: 0.0001, loss G: 9.7785\n",
      "Epoch [1/10] Batch 350/1583                   Loss D: 0.0001, loss G: 9.7820\n",
      "Epoch [1/10] Batch 360/1583                   Loss D: 0.0001, loss G: 9.8077\n",
      "Epoch [1/10] Batch 370/1583                   Loss D: 0.0001, loss G: 9.8136\n",
      "Epoch [1/10] Batch 380/1583                   Loss D: 0.0001, loss G: 9.8344\n",
      "Epoch [1/10] Batch 390/1583                   Loss D: 0.0000, loss G: 9.8249\n",
      "Epoch [1/10] Batch 400/1583                   Loss D: 0.0001, loss G: 9.8377\n",
      "Epoch [1/10] Batch 410/1583                   Loss D: 0.0000, loss G: 9.8572\n",
      "Epoch [1/10] Batch 420/1583                   Loss D: 0.0000, loss G: 9.8815\n",
      "Epoch [1/10] Batch 430/1583                   Loss D: 0.0001, loss G: 9.8845\n",
      "Epoch [1/10] Batch 440/1583                   Loss D: 0.0000, loss G: 9.9142\n",
      "Epoch [1/10] Batch 450/1583                   Loss D: 0.0000, loss G: 9.9316\n",
      "Epoch [1/10] Batch 460/1583                   Loss D: 0.0000, loss G: 9.9469\n",
      "Epoch [1/10] Batch 470/1583                   Loss D: 0.0000, loss G: 9.9516\n",
      "Epoch [1/10] Batch 480/1583                   Loss D: 0.0000, loss G: 9.9575\n",
      "Epoch [1/10] Batch 490/1583                   Loss D: 0.0000, loss G: 9.9709\n",
      "Epoch [1/10] Batch 500/1583                   Loss D: 0.0000, loss G: 9.9807\n",
      "Epoch [1/10] Batch 510/1583                   Loss D: 0.0000, loss G: 9.9942\n",
      "Epoch [1/10] Batch 520/1583                   Loss D: 0.0000, loss G: 10.0074\n",
      "Epoch [1/10] Batch 530/1583                   Loss D: 0.0000, loss G: 10.0128\n",
      "Epoch [1/10] Batch 540/1583                   Loss D: 0.0000, loss G: 10.0403\n",
      "Epoch [1/10] Batch 550/1583                   Loss D: 0.0000, loss G: 10.0513\n",
      "Epoch [1/10] Batch 560/1583                   Loss D: 0.0000, loss G: 10.0691\n",
      "Epoch [1/10] Batch 570/1583                   Loss D: 0.0000, loss G: 10.0749\n",
      "Epoch [1/10] Batch 580/1583                   Loss D: 0.0000, loss G: 10.0940\n",
      "Epoch [1/10] Batch 590/1583                   Loss D: 0.0000, loss G: 10.1223\n",
      "Epoch [1/10] Batch 600/1583                   Loss D: 0.0000, loss G: 10.1327\n",
      "Epoch [1/10] Batch 610/1583                   Loss D: 0.0000, loss G: 10.1451\n",
      "Epoch [1/10] Batch 620/1583                   Loss D: 0.0000, loss G: 10.1643\n",
      "Epoch [1/10] Batch 630/1583                   Loss D: 0.0000, loss G: 10.1777\n",
      "Epoch [1/10] Batch 640/1583                   Loss D: 0.0000, loss G: 10.1931\n",
      "Epoch [1/10] Batch 650/1583                   Loss D: 0.0000, loss G: 10.2091\n",
      "Epoch [1/10] Batch 660/1583                   Loss D: 0.0000, loss G: 10.2181\n",
      "Epoch [1/10] Batch 670/1583                   Loss D: 0.0000, loss G: 10.2439\n",
      "Epoch [1/10] Batch 680/1583                   Loss D: 0.0000, loss G: 10.2590\n",
      "Epoch [1/10] Batch 690/1583                   Loss D: 0.0000, loss G: 10.2730\n",
      "Epoch [1/10] Batch 700/1583                   Loss D: 0.0000, loss G: 10.2859\n",
      "Epoch [1/10] Batch 710/1583                   Loss D: 0.0000, loss G: 10.2941\n",
      "Epoch [1/10] Batch 720/1583                   Loss D: 0.0000, loss G: 10.3074\n",
      "Epoch [1/10] Batch 730/1583                   Loss D: 0.0000, loss G: 10.3228\n",
      "Epoch [1/10] Batch 740/1583                   Loss D: 0.0000, loss G: 10.3396\n",
      "Epoch [1/10] Batch 750/1583                   Loss D: 0.0000, loss G: 10.3428\n",
      "Epoch [1/10] Batch 760/1583                   Loss D: 0.0000, loss G: 10.3727\n",
      "Epoch [1/10] Batch 770/1583                   Loss D: 0.0000, loss G: 10.3755\n",
      "Epoch [1/10] Batch 780/1583                   Loss D: 0.0000, loss G: 10.3902\n",
      "Epoch [1/10] Batch 790/1583                   Loss D: 0.0000, loss G: 10.4051\n",
      "Epoch [1/10] Batch 800/1583                   Loss D: 0.0000, loss G: 10.4184\n",
      "Epoch [1/10] Batch 810/1583                   Loss D: 0.0000, loss G: 10.4379\n",
      "Epoch [1/10] Batch 820/1583                   Loss D: 0.0000, loss G: 10.4474\n",
      "Epoch [1/10] Batch 830/1583                   Loss D: 0.0000, loss G: 10.4680\n",
      "Epoch [1/10] Batch 840/1583                   Loss D: 0.0000, loss G: 10.4836\n",
      "Epoch [1/10] Batch 850/1583                   Loss D: 0.0000, loss G: 10.4858\n",
      "Epoch [1/10] Batch 860/1583                   Loss D: 0.0000, loss G: 10.4955\n",
      "Epoch [1/10] Batch 870/1583                   Loss D: 0.0000, loss G: 10.5162\n",
      "Epoch [1/10] Batch 880/1583                   Loss D: 0.0000, loss G: 10.5212\n",
      "Epoch [1/10] Batch 890/1583                   Loss D: 0.0000, loss G: 10.5337\n",
      "Epoch [1/10] Batch 900/1583                   Loss D: 0.0000, loss G: 10.5512\n",
      "Epoch [1/10] Batch 910/1583                   Loss D: 0.0000, loss G: 10.5603\n",
      "Epoch [1/10] Batch 920/1583                   Loss D: 0.0000, loss G: 10.5764\n",
      "Epoch [1/10] Batch 930/1583                   Loss D: 0.0000, loss G: 10.5885\n",
      "Epoch [1/10] Batch 940/1583                   Loss D: 0.0000, loss G: 10.6052\n",
      "Epoch [1/10] Batch 950/1583                   Loss D: 0.0000, loss G: 10.6191\n",
      "Epoch [1/10] Batch 960/1583                   Loss D: 0.0000, loss G: 10.6247\n",
      "Epoch [1/10] Batch 970/1583                   Loss D: 0.0000, loss G: 10.6402\n",
      "Epoch [1/10] Batch 980/1583                   Loss D: 0.0000, loss G: 10.6483\n",
      "Epoch [1/10] Batch 990/1583                   Loss D: 0.0000, loss G: 10.6735\n",
      "Epoch [1/10] Batch 1000/1583                   Loss D: 0.0000, loss G: 10.6826\n",
      "Epoch [1/10] Batch 1010/1583                   Loss D: 0.0000, loss G: 10.6892\n",
      "Epoch [1/10] Batch 1020/1583                   Loss D: 0.0000, loss G: 10.6999\n",
      "Epoch [1/10] Batch 1030/1583                   Loss D: 0.0000, loss G: 10.7147\n",
      "Epoch [1/10] Batch 1040/1583                   Loss D: 0.0000, loss G: 10.7265\n",
      "Epoch [1/10] Batch 1050/1583                   Loss D: 0.0000, loss G: 10.7403\n",
      "Epoch [1/10] Batch 1060/1583                   Loss D: 0.0000, loss G: 10.7492\n",
      "Epoch [1/10] Batch 1070/1583                   Loss D: 0.0000, loss G: 10.7645\n",
      "Epoch [1/10] Batch 1080/1583                   Loss D: 0.0000, loss G: 10.7784\n",
      "Epoch [1/10] Batch 1090/1583                   Loss D: 0.0000, loss G: 10.7909\n",
      "Epoch [1/10] Batch 1100/1583                   Loss D: 0.0000, loss G: 10.7997\n",
      "Epoch [1/10] Batch 1110/1583                   Loss D: 0.0000, loss G: 10.8110\n",
      "Epoch [1/10] Batch 1120/1583                   Loss D: 0.0000, loss G: 10.8328\n",
      "Epoch [1/10] Batch 1130/1583                   Loss D: 0.0000, loss G: 10.8428\n",
      "Epoch [1/10] Batch 1140/1583                   Loss D: 0.0000, loss G: 10.8541\n",
      "Epoch [1/10] Batch 1150/1583                   Loss D: 0.0000, loss G: 10.8695\n",
      "Epoch [1/10] Batch 1160/1583                   Loss D: 0.0000, loss G: 10.8751\n",
      "Epoch [1/10] Batch 1170/1583                   Loss D: 0.0000, loss G: 10.8828\n",
      "Epoch [1/10] Batch 1180/1583                   Loss D: 0.0000, loss G: 10.8931\n",
      "Epoch [1/10] Batch 1190/1583                   Loss D: 0.0000, loss G: 10.9067\n",
      "Epoch [1/10] Batch 1200/1583                   Loss D: 0.0000, loss G: 10.9124\n",
      "Epoch [1/10] Batch 1210/1583                   Loss D: 0.0000, loss G: 10.9248\n",
      "Epoch [1/10] Batch 1220/1583                   Loss D: 0.0000, loss G: 10.9312\n",
      "Epoch [1/10] Batch 1230/1583                   Loss D: 0.0000, loss G: 10.9411\n",
      "Epoch [1/10] Batch 1240/1583                   Loss D: 0.0000, loss G: 10.9456\n",
      "Epoch [1/10] Batch 1250/1583                   Loss D: 0.0000, loss G: 10.9536\n",
      "Epoch [1/10] Batch 1260/1583                   Loss D: 0.0000, loss G: 10.9581\n",
      "Epoch [1/10] Batch 1270/1583                   Loss D: 0.0000, loss G: 10.9632\n",
      "Epoch [1/10] Batch 1280/1583                   Loss D: 0.0000, loss G: 10.9678\n",
      "Epoch [1/10] Batch 1290/1583                   Loss D: 0.0000, loss G: 10.9683\n",
      "Epoch [1/10] Batch 1300/1583                   Loss D: 0.0000, loss G: 10.9827\n",
      "Epoch [1/10] Batch 1310/1583                   Loss D: 0.0000, loss G: 10.9824\n",
      "Epoch [1/10] Batch 1320/1583                   Loss D: 0.0000, loss G: 10.9843\n",
      "Epoch [1/10] Batch 1330/1583                   Loss D: 0.0000, loss G: 10.9942\n",
      "Epoch [1/10] Batch 1340/1583                   Loss D: 0.0000, loss G: 10.9914\n",
      "Epoch [1/10] Batch 1350/1583                   Loss D: 0.0000, loss G: 10.9933\n",
      "Epoch [1/10] Batch 1360/1583                   Loss D: 0.0000, loss G: 11.0014\n",
      "Epoch [1/10] Batch 1370/1583                   Loss D: 0.0000, loss G: 10.9963\n",
      "Epoch [1/10] Batch 1380/1583                   Loss D: 0.0000, loss G: 11.0027\n",
      "Epoch [1/10] Batch 1390/1583                   Loss D: 0.0000, loss G: 10.9967\n",
      "Epoch [1/10] Batch 1400/1583                   Loss D: 0.0000, loss G: 11.0091\n",
      "Epoch [1/10] Batch 1410/1583                   Loss D: 0.0000, loss G: 11.0102\n",
      "Epoch [1/10] Batch 1420/1583                   Loss D: 0.0000, loss G: 11.0103\n",
      "Epoch [1/10] Batch 1430/1583                   Loss D: 0.0000, loss G: 11.0194\n",
      "Epoch [1/10] Batch 1440/1583                   Loss D: 0.0000, loss G: 11.0170\n",
      "Epoch [1/10] Batch 1450/1583                   Loss D: 0.0000, loss G: 11.0267\n",
      "Epoch [1/10] Batch 1460/1583                   Loss D: 0.0000, loss G: 11.0212\n",
      "Epoch [1/10] Batch 1470/1583                   Loss D: 0.0000, loss G: 11.0234\n",
      "Epoch [1/10] Batch 1480/1583                   Loss D: 0.0000, loss G: 11.0289\n",
      "Epoch [1/10] Batch 1490/1583                   Loss D: 0.0000, loss G: 11.0304\n",
      "Epoch [1/10] Batch 1500/1583                   Loss D: 0.0000, loss G: 11.0255\n",
      "Epoch [1/10] Batch 1510/1583                   Loss D: 0.0000, loss G: 11.0297\n",
      "Epoch [1/10] Batch 1520/1583                   Loss D: 0.0000, loss G: 11.0241\n",
      "Epoch [1/10] Batch 1530/1583                   Loss D: 0.0000, loss G: 11.0232\n",
      "Epoch [1/10] Batch 1540/1583                   Loss D: 0.0000, loss G: 11.0325\n",
      "Epoch [1/10] Batch 1550/1583                   Loss D: 0.0000, loss G: 11.0366\n",
      "Epoch [1/10] Batch 1560/1583                   Loss D: 0.0000, loss G: 11.0343\n",
      "Epoch [1/10] Batch 1570/1583                   Loss D: 0.0000, loss G: 11.0435\n",
      "Epoch [1/10] Batch 1580/1583                   Loss D: 0.0000, loss G: 11.0436\n",
      "Epoch [2/10] Batch 0/1583                   Loss D: 0.0000, loss G: 11.0422\n",
      "Epoch [2/10] Batch 10/1583                   Loss D: 0.0000, loss G: 11.0519\n",
      "Epoch [2/10] Batch 20/1583                   Loss D: 0.0000, loss G: 11.0593\n",
      "Epoch [2/10] Batch 30/1583                   Loss D: 0.0000, loss G: 11.0636\n",
      "Epoch [2/10] Batch 40/1583                   Loss D: 0.0000, loss G: 11.0748\n",
      "Epoch [2/10] Batch 50/1583                   Loss D: 0.0000, loss G: 11.0757\n",
      "Epoch [2/10] Batch 60/1583                   Loss D: 0.0000, loss G: 11.0910\n",
      "Epoch [2/10] Batch 70/1583                   Loss D: 0.0000, loss G: 11.0948\n",
      "Epoch [2/10] Batch 80/1583                   Loss D: 0.0000, loss G: 11.1036\n",
      "Epoch [2/10] Batch 90/1583                   Loss D: 0.0000, loss G: 11.1130\n",
      "Epoch [2/10] Batch 100/1583                   Loss D: 0.0000, loss G: 11.1163\n",
      "Epoch [2/10] Batch 110/1583                   Loss D: 0.0000, loss G: 11.1226\n",
      "Epoch [2/10] Batch 120/1583                   Loss D: 0.0000, loss G: 11.1381\n",
      "Epoch [2/10] Batch 130/1583                   Loss D: 0.0000, loss G: 11.1361\n",
      "Epoch [2/10] Batch 140/1583                   Loss D: 0.0000, loss G: 11.1477\n",
      "Epoch [2/10] Batch 150/1583                   Loss D: 0.0000, loss G: 11.1644\n",
      "Epoch [2/10] Batch 160/1583                   Loss D: 0.0000, loss G: 11.1735\n",
      "Epoch [2/10] Batch 170/1583                   Loss D: 0.0000, loss G: 11.1853\n",
      "Epoch [2/10] Batch 180/1583                   Loss D: 0.0000, loss G: 11.1924\n",
      "Epoch [2/10] Batch 190/1583                   Loss D: 0.0000, loss G: 11.2019\n",
      "Epoch [2/10] Batch 200/1583                   Loss D: 0.0000, loss G: 11.2123\n",
      "Epoch [2/10] Batch 210/1583                   Loss D: 0.0000, loss G: 11.2175\n",
      "Epoch [2/10] Batch 220/1583                   Loss D: 0.0000, loss G: 11.2311\n",
      "Epoch [2/10] Batch 230/1583                   Loss D: 0.0000, loss G: 11.2483\n",
      "Epoch [2/10] Batch 240/1583                   Loss D: 0.0000, loss G: 11.2551\n",
      "Epoch [2/10] Batch 250/1583                   Loss D: 0.0000, loss G: 11.2651\n",
      "Epoch [2/10] Batch 260/1583                   Loss D: 0.0000, loss G: 11.2768\n",
      "Epoch [2/10] Batch 270/1583                   Loss D: 0.0000, loss G: 11.2811\n",
      "Epoch [2/10] Batch 280/1583                   Loss D: 0.0000, loss G: 11.2922\n",
      "Epoch [2/10] Batch 290/1583                   Loss D: 0.0000, loss G: 11.3027\n",
      "Epoch [2/10] Batch 300/1583                   Loss D: 0.0000, loss G: 11.3109\n",
      "Epoch [2/10] Batch 310/1583                   Loss D: 0.0000, loss G: 11.3232\n",
      "Epoch [2/10] Batch 320/1583                   Loss D: 0.0000, loss G: 11.3287\n",
      "Epoch [2/10] Batch 330/1583                   Loss D: 0.0000, loss G: 11.3460\n",
      "Epoch [2/10] Batch 340/1583                   Loss D: 0.0000, loss G: 11.3483\n",
      "Epoch [2/10] Batch 350/1583                   Loss D: 0.0000, loss G: 11.3667\n",
      "Epoch [2/10] Batch 360/1583                   Loss D: 0.0000, loss G: 11.3738\n",
      "Epoch [2/10] Batch 370/1583                   Loss D: 0.0000, loss G: 11.3814\n",
      "Epoch [2/10] Batch 380/1583                   Loss D: 0.0000, loss G: 11.3870\n",
      "Epoch [2/10] Batch 390/1583                   Loss D: 0.0000, loss G: 11.4028\n",
      "Epoch [2/10] Batch 400/1583                   Loss D: 0.0000, loss G: 11.4101\n",
      "Epoch [2/10] Batch 410/1583                   Loss D: 0.0000, loss G: 11.4197\n",
      "Epoch [2/10] Batch 420/1583                   Loss D: 0.0000, loss G: 11.4237\n",
      "Epoch [2/10] Batch 430/1583                   Loss D: 0.0000, loss G: 11.4369\n",
      "Epoch [2/10] Batch 440/1583                   Loss D: 0.0000, loss G: 11.4482\n",
      "Epoch [2/10] Batch 450/1583                   Loss D: 0.0000, loss G: 11.4625\n",
      "Epoch [2/10] Batch 460/1583                   Loss D: 0.0000, loss G: 11.4703\n",
      "Epoch [2/10] Batch 470/1583                   Loss D: 0.0000, loss G: 11.4744\n",
      "Epoch [2/10] Batch 480/1583                   Loss D: 0.0000, loss G: 11.4825\n",
      "Epoch [2/10] Batch 490/1583                   Loss D: 0.0000, loss G: 11.4910\n",
      "Epoch [2/10] Batch 500/1583                   Loss D: 0.0000, loss G: 11.5073\n",
      "Epoch [2/10] Batch 510/1583                   Loss D: 0.0000, loss G: 11.5167\n",
      "Epoch [2/10] Batch 520/1583                   Loss D: 0.0000, loss G: 11.5241\n",
      "Epoch [2/10] Batch 530/1583                   Loss D: 0.0000, loss G: 11.5327\n",
      "Epoch [2/10] Batch 540/1583                   Loss D: 0.0000, loss G: 11.5400\n",
      "Epoch [2/10] Batch 550/1583                   Loss D: 0.0000, loss G: 11.5542\n",
      "Epoch [2/10] Batch 560/1583                   Loss D: 0.0000, loss G: 11.5637\n",
      "Epoch [2/10] Batch 570/1583                   Loss D: 0.0000, loss G: 11.5693\n",
      "Epoch [2/10] Batch 580/1583                   Loss D: 0.0000, loss G: 11.5812\n",
      "Epoch [2/10] Batch 590/1583                   Loss D: 0.0000, loss G: 11.5938\n",
      "Epoch [2/10] Batch 600/1583                   Loss D: 0.0000, loss G: 11.6058\n",
      "Epoch [2/10] Batch 610/1583                   Loss D: 0.0000, loss G: 11.6087\n",
      "Epoch [2/10] Batch 620/1583                   Loss D: 0.0000, loss G: 11.6193\n",
      "Epoch [2/10] Batch 630/1583                   Loss D: 0.0000, loss G: 11.6264\n",
      "Epoch [2/10] Batch 640/1583                   Loss D: 0.0000, loss G: 11.6398\n",
      "Epoch [2/10] Batch 650/1583                   Loss D: 0.0000, loss G: 11.6524\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(NUM_EPOCHS):\n\u001b[1;32m----> 7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (real, _) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(loader):\n\u001b[0;32m      8\u001b[0m         real \u001b[38;5;241m=\u001b[39m real\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      9\u001b[0m         noise \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(real\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), Z_DIM, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# Adjusted to use real.size(0)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\fyzan\\anaconda3\\envs\\isl\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\fyzan\\anaconda3\\envs\\isl\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\fyzan\\anaconda3\\envs\\isl\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\fyzan\\anaconda3\\envs\\isl\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\fyzan\\anaconda3\\envs\\isl\\lib\\site-packages\\torchvision\\datasets\\folder.py:245\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;124;03m    index (int): Index\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;124;03m    tuple: (sample, target) where target is class_index of the target class.\u001b[39;00m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    244\u001b[0m path, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples[index]\n\u001b[1;32m--> 245\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    247\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(sample)\n",
      "File \u001b[1;32mc:\\Users\\fyzan\\anaconda3\\envs\\isl\\lib\\site-packages\\torchvision\\datasets\\folder.py:284\u001b[0m, in \u001b[0;36mdefault_loader\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m accimage_loader(path)\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpil_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\fyzan\\anaconda3\\envs\\isl\\lib\\site-packages\\torchvision\\datasets\\folder.py:263\u001b[0m, in \u001b[0;36mpil_loader\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpil_loader\u001b[39m(path: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Image\u001b[38;5;241m.\u001b[39mImage:\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;66;03m# open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\u001b[39;00m\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m--> 263\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    264\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m img\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\fyzan\\anaconda3\\envs\\isl\\lib\\site-packages\\PIL\\Image.py:3225\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3217\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pixels \u001b[38;5;241m>\u001b[39m MAX_IMAGE_PIXELS:\n\u001b[0;32m   3218\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   3219\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpixels\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m pixels) exceeds limit of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMAX_IMAGE_PIXELS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m pixels, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3220\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould be decompression bomb DOS attack.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3221\u001b[0m             DecompressionBombWarning,\n\u001b[0;32m   3222\u001b[0m         )\n\u001b[1;32m-> 3225\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen\u001b[39m(fp, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, formats\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Image:\n\u001b[0;32m   3226\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3227\u001b[0m \u001b[38;5;124;03m    Opens and identifies the given image file.\u001b[39;00m\n\u001b[0;32m   3228\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3252\u001b[0m \u001b[38;5;124;03m    :exception TypeError: If ``formats`` is not ``None``, a list or a tuple.\u001b[39;00m\n\u001b[0;32m   3253\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   3255\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define variables\n",
    "accumulation_steps = 2  # Gradient accumulation steps\n",
    "total_steps = len(loader)\n",
    "step = 0\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for batch_idx, (real, _) in enumerate(loader):\n",
    "        real = real.to(device)\n",
    "        noise = torch.randn(real.size(0), Z_DIM, 1, 1).to(device)  # Adjusted to use real.size(0)\n",
    "        fake = gen(noise)\n",
    "\n",
    "        ### Train Discriminator\n",
    "        disc.zero_grad()\n",
    "        disc_real = disc(real).view(-1)\n",
    "        loss_disc_real = criterion(disc_real, torch.ones_like(disc_real))\n",
    "        disc_fake = disc(fake.detach()).view(-1)\n",
    "        loss_disc_fake = criterion(disc_fake, torch.zeros_like(disc_fake))\n",
    "        loss_disc = (loss_disc_real + loss_disc_fake) / 2\n",
    "\n",
    "        loss_disc.backward()\n",
    "        opt_disc.step()\n",
    "\n",
    "        ### Train Generator\n",
    "        if batch_idx % accumulation_steps == 0:\n",
    "            gen.zero_grad()\n",
    "            output = disc(fake).view(-1)\n",
    "            loss_gen = criterion(output, torch.ones_like(output))\n",
    "            loss_gen.backward()\n",
    "\n",
    "            opt_gen.step()\n",
    "\n",
    "        # Gradient accumulation\n",
    "        if batch_idx % accumulation_steps == 0:\n",
    "            opt_gen.step()\n",
    "\n",
    "        # Print losses occasionally\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch}/{NUM_EPOCHS}] Batch {batch_idx}/{total_steps} \\\n",
    "                  Loss D: {loss_disc:.4f}, loss G: {loss_gen:.4f}\"\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                fake = gen(fixed_noise)\n",
    "                img_grid_real = torchvision.utils.make_grid(\n",
    "                    real[:32], normalize=True\n",
    "                )\n",
    "                img_grid_fake = torchvision.utils.make_grid(\n",
    "                    fake[:32], normalize=True\n",
    "                )\n",
    "                writer_real.add_image(\"Real Celeb\", img_grid_real, global_step=step)\n",
    "                writer_fake.add_image(\"Fake Celeb\", img_grid_fake, global_step=step)\n",
    "\n",
    "            step += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WGAN implementation from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, channels_img, features_d):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            nn.Conv2d(channels_img, features_d, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            self._block(features_d, features_d * 2, 4, 2, 1),\n",
    "            self._block(features_d * 2, features_d * 4, 4, 2, 1),\n",
    "            self._block(features_d * 4, features_d * 8, 4, 2, 1),\n",
    "            nn.Conv2d(features_d * 8, 1, kernel_size=4, stride=2, padding=0, bias=False),\n",
    "        )\n",
    "\n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.disc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settign the hyperparameters\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "LEARNING_RATE = 5e-5\n",
    "BATCH_SIZE = 128\n",
    "IMAGE_SIZE = 64\n",
    "# CHANNELS_IMG = 1\n",
    "CHANNELS_IMG = 3\n",
    "Z_DIM = 100\n",
    "NUM_EPOCHS = 10\n",
    "FEATURES_DISC = 64\n",
    "FEATURES_GEN = 64\n",
    "CRITIC_ITERATIONS =5\n",
    "WEIGHT_CLIP = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transforms = transforms.Compose([\n",
    "    transforms.Resize(IMAGE_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        [0.5 for _ in range(CHANNELS_IMG)], [0.5 for _ in range(CHANNELS_IMG)]),\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.ImageFolder(\n",
    "    root=\"celeb_dataset\", transform=transforms )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "gen = Generator(Z_DIM, CHANNELS_IMG, FEATURES_GEN).to(device)\n",
    "critic = Discriminator(CHANNELS_IMG, FEATURES_DISC).to(device)\n",
    "\n",
    "initialize_weights(gen)\n",
    "initialize_weights(critic)\n",
    "\n",
    "opt_gen = optim.RMSprop(gen.parameters(), lr=LEARNING_RATE)\n",
    "opt_critic = optim.RMSprop(critic.parameters(), lr=LEARNING_RATE) \n",
    "fixed_noise = torch.randn(32, Z_DIM, 1, 1).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer_real = SummaryWriter(f\"logs/real celeb\")\n",
    "writer_fake = SummaryWriter(f\"logs/fake celeb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 0\n",
    "\n",
    "gen.train()\n",
    "critic.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    for batch_idx, (real, _) in enumerate(loader):\n",
    "        real = real.to(device)\n",
    "        noise = torch.randn(BATCH_SIZE, Z_DIM, 1, 1).to(device)\n",
    "        fake = gen(noise)\n",
    "\n",
    "        for _ in range(CRITIC_ITERATIONS):\n",
    "            noise = torch.randn(BATCH_SIZE, Z_DIM, 1, 1).to(device)\n",
    "            fake = gen(noise)\n",
    "            critic_real = critic(real).reshapr(-1)\n",
    "            critic_fake = critic(fake).reshape(-1)\n",
    "            loss_critic = -torch.mean(critic_real) + torch.mean(critic_fake)\n",
    "            critic.zero_grad()\n",
    "            loss_critic.backward(retain_graph=True)\n",
    "\n",
    "\n",
    "        ### Train Discriminator: max log(D(x)) + log(1 - D(G(z)))\n",
    "        disc.zero_grad()\n",
    "        disc_real = disc(real).reshape(-1)\n",
    "        loss_disc_real = criterion(disc_real, torch.ones_like(disc_real))\n",
    "        disc_fake = disc(fake.detach()).reshape(-1)\n",
    "        loss_disc_fake = criterion(disc_fake, torch.zeros_like(disc_fake))\n",
    "        loss_disc = (loss_disc_real + loss_disc_fake) / 2\n",
    "        loss_disc.backward(retain_graph=True)\n",
    "        opt_disc.step()\n",
    "\n",
    "        for p in critic.parameters():\n",
    "            p.data.clamp_(-WEIGHT_CLIP, WEIGHT_CLIP)\n",
    "\n",
    "\n",
    "        ### Train Generator: \n",
    "        gen.zero_grad()\n",
    "        output = critic(fake).reshape(-1)\n",
    "        loss_gen = -torch.mean(output)\n",
    "        loss_gen.backward()\n",
    "        opt_gen.step()\n",
    "\n",
    "        # print losses occassionally and print to tensorboard\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch}/{NUM_EPOCHS}] Batch {batch_idx}/{len(loader)} \\\n",
    "                  Loss D: {loss_critic:.4f}, loss G: {loss_gen:.4f}\"\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                fake = gen(fixed_noise)\n",
    "                img_grid_real = torchvision.utils.make_grid(\n",
    "                    real[:32], normalize=True\n",
    "                )\n",
    "                img_grid_fake = torchvision.utils.make_grid(\n",
    "                    fake[:32], normalize=True\n",
    "                )\n",
    "                writer_real.add_image(\"Real Celeb\", img_grid_real, global_step=step)\n",
    "                writer_fake.add_image(\"Fake Celeb\", img_grid_fake, global_step=step)\n",
    "\n",
    "            step += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "isl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
